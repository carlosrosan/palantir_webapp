{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicci√≥n de Fallas con Decision Tree y LightGBM\n",
    "\n",
    "Este notebook aplica modelos de clasificaci√≥n para predecir fallas usando datos de la tabla `faliure_probability_base`.\n",
    "\n",
    "**Caracter√≠sticas del dataset:**\n",
    "- Granularidad diaria (`reading_date`)\n",
    "- Variable objetivo: `faliure` (booleano indicando si hay falla en los pr√≥ximos 7 d√≠as)\n",
    "\n",
    "**Modelos utilizados:**\n",
    "- √Årbol de Decisi√≥n (Decision Tree)\n",
    "- LightGBM\n",
    "\n",
    "Este notebook cubre:\n",
    "- Carga de datos desde MySQL\n",
    "- Preparaci√≥n y feature engineering\n",
    "- Entrenamiento de modelos\n",
    "- Evaluaci√≥n y m√©tricas\n",
    "- An√°lisis de importancia de caracter√≠sticas\n",
    "- Predicciones y an√°lisis de resultados\n",
    "- Guardar y cargar modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Importaci√≥n de Librer√≠as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import mysql.connector\n",
    "from mysql.connector import Error\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    classification_report, confusion_matrix,\n",
    "    roc_auc_score, roc_curve\n",
    ")\n",
    "from lightgbm import LGBMClassifier\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"Librer√≠as importadas correctamente\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Carga de Datos desde MySQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuraci√≥n de la base de datos\n",
    "DB_CONFIG = {\n",
    "    'host': '127.0.0.1',\n",
    "    'database': 'palantir_maintenance',\n",
    "    'user': 'root',\n",
    "    'password': 'admin',\n",
    "    'port': 3306\n",
    "}\n",
    "\n",
    "def cargar_datos():\n",
    "    \"\"\"Cargar datos de la tabla faliure_probability_base (misma funci√≥n que otros notebooks)\"\"\"\n",
    "    try:\n",
    "        connection = mysql.connector.connect(**DB_CONFIG)\n",
    "        if connection.is_connected():\n",
    "            query = \"\"\"\n",
    "            SELECT * \n",
    "            FROM faliure_probability_base\n",
    "            ORDER BY asset_id, reading_date\n",
    "            \"\"\"\n",
    "            df = pd.read_sql(query, connection)\n",
    "            connection.close()\n",
    "            print(f\"‚úÖ Datos cargados: {df.shape[0]} filas, {df.shape[1]} columnas\")\n",
    "            return df\n",
    "    except Error as e:\n",
    "        print(f\"‚ùå Error al conectar: {e}\")\n",
    "        return None\n",
    "\n",
    "# Cargar datos (usando la misma funci√≥n que los dem√°s notebooks)\n",
    "df = cargar_datos()\n",
    "\n",
    "if df is not None and len(df) > 0:\n",
    "    print(f\"\\nColumnas disponibles: {list(df.columns)}\")\n",
    "    print(f\"\\nPrimeras filas:\")\n",
    "    display(df.head())\n",
    "    print(f\"\\nInformaci√≥n del DataFrame:\")\n",
    "    print(df.info())\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No se pudieron cargar los datos. Verifica la conexi√≥n a la base de datos.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. An√°lisis Exploratorio de Datos (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if df is not None and 'faliure' in df.columns:\n",
    "    # An√°lisis de la variable objetivo\n",
    "    print(\"Distribuci√≥n de la variable objetivo 'faliure':\")\n",
    "    faliure_dist = df['faliure'].value_counts()\n",
    "    print(faliure_dist)\n",
    "    print(f\"\\nTasa de fallas: {df['faliure'].mean()*100:.2f}%\")\n",
    "    \n",
    "    # Visualizaci√≥n\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    \n",
    "    # Gr√°fico 1: Distribuci√≥n de fallas\n",
    "    axes[0, 0].bar(['Sin Falla', 'Con Falla'], faliure_dist.values, color=['green', 'red'])\n",
    "    axes[0, 0].set_title('Distribuci√≥n de Fallas en Pr√≥ximos 7 D√≠as')\n",
    "    axes[0, 0].set_ylabel('Frecuencia')\n",
    "    \n",
    "    # Gr√°fico 2: Fallas por activo\n",
    "    fallas_por_activo = df.groupby('asset_id')['faliure'].sum()\n",
    "    axes[0, 1].bar(fallas_por_activo.index, fallas_por_activo.values)\n",
    "    axes[0, 1].set_title('Fallas por Activo')\n",
    "    axes[0, 1].set_xlabel('Asset ID')\n",
    "    axes[0, 1].set_ylabel('N√∫mero de d√≠as con falla pr√≥xima')\n",
    "    \n",
    "    # Gr√°fico 3: Distribuci√≥n de sensor_warning_count\n",
    "    if 'sensor_warning_count_30d' in df.columns:\n",
    "        df['sensor_warning_count_30d'].fillna(0).hist(bins=20, ax=axes[1, 0])\n",
    "        axes[1, 0].set_title('Distribuci√≥n de Alertas de Sensor (30 d√≠as)')\n",
    "        axes[1, 0].set_xlabel('Conteo de Alertas')\n",
    "        axes[1, 0].set_ylabel('Frecuencia')\n",
    "    \n",
    "    # Gr√°fico 4: Distribuci√≥n de failure_count\n",
    "    if 'failure_count_365d' in df.columns:\n",
    "        df['failure_count_365d'].fillna(0).hist(bins=20, ax=axes[1, 1])\n",
    "        axes[1, 1].set_title('Distribuci√≥n de Fallas (365 d√≠as)')\n",
    "        axes[1, 1].set_xlabel('Conteo de Fallas')\n",
    "        axes[1, 1].set_ylabel('Frecuencia')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No hay datos o columna 'faliure' no encontrada\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Preparaci√≥n de Caracter√≠sticas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if df is not None and 'faliure' in df.columns:\n",
    "    # Columnas a excluir\n",
    "    exclude_cols = ['base_id', 'asset_id', 'reading_date', 'faliure', \n",
    "                    'asset_status', 'created_at', 'updated_at']\n",
    "    \n",
    "    # Seleccionar caracter√≠sticas num√©ricas\n",
    "    feature_columns = [col for col in df.columns \n",
    "                       if col not in exclude_cols and df[col].dtype in ['int64', 'float64']]\n",
    "    \n",
    "    print(f\"Caracter√≠sticas seleccionadas: {len(feature_columns)}\")\n",
    "    print(f\"\\nColumnas: {feature_columns}\")\n",
    "    \n",
    "    # Preparar datos\n",
    "    X = df[feature_columns].fillna(0).replace([np.inf, -np.inf], 0)\n",
    "    y = df['faliure'].astype(int)\n",
    "    \n",
    "    print(f\"\\nForma de X: {X.shape}\")\n",
    "    print(f\"Forma de y: {y.shape}\")\n",
    "    print(f\"\\nDistribuci√≥n de clases:\")\n",
    "    print(y.value_counts())\n",
    "    \n",
    "    if len(X) < 50:\n",
    "        print(\"‚ö†Ô∏è ADVERTENCIA: Muy pocos datos para entrenar un modelo robusto\")\n",
    "    else:\n",
    "        print(f\"\\n‚úÖ Datos suficientes para entrenamiento: {len(X)} muestras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Divisi√≥n de Datos en Train y Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'X' in locals() and 'y' in locals() and len(np.unique(y)) > 1:\n",
    "    # Dividir datos\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "    \n",
    "    print(f\"Datos de entrenamiento: {X_train.shape}\")\n",
    "    print(f\"Datos de prueba: {X_test.shape}\")\n",
    "    \n",
    "    print(f\"\\nDistribuci√≥n de clases en entrenamiento:\")\n",
    "    print(f\"  Sin falla: {(y_train == 0).sum()} ({(y_train == 0).sum()/len(y_train)*100:.1f}%)\")\n",
    "    print(f\"  Con falla: {(y_train == 1).sum()} ({(y_train == 1).sum()/len(y_train)*100:.1f}%)\")\n",
    "    \n",
    "    print(f\"\\nDistribuci√≥n de clases en prueba:\")\n",
    "    print(f\"  Sin falla: {(y_test == 0).sum()} ({(y_test == 0).sum()/len(y_test)*100:.1f}%)\")\n",
    "    print(f\"  Con falla: {(y_test == 1).sum()} ({(y_test == 1).sum()/len(y_test)*100:.1f}%)\")\n",
    "    \n",
    "    # Escalar caracter√≠sticas\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # Convertir a DataFrame para LightGBM\n",
    "    X_train_scaled_df = pd.DataFrame(X_train_scaled, columns=X_train.columns)\n",
    "    X_test_scaled_df = pd.DataFrame(X_test_scaled, columns=X_test.columns)\n",
    "    \n",
    "    print(\"\\n‚úÖ Datos divididos y escalados correctamente\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No hay datos preparados o solo hay una clase\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Entrenamiento del Modelo Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'X_train' in locals():\n",
    "    print(\"=\"*60)\n",
    "    print(\"ENTRENANDO √ÅRBOL DE DECISI√ìN\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Crear modelo\n",
    "    dt_model = DecisionTreeClassifier(\n",
    "        max_depth=10,\n",
    "        min_samples_split=5,\n",
    "        min_samples_leaf=2,\n",
    "        class_weight='balanced',\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    # Entrenar\n",
    "    dt_model.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # Predicciones\n",
    "    y_pred_dt = dt_model.predict(X_test_scaled)\n",
    "    y_pred_proba_dt = dt_model.predict_proba(X_test_scaled)[:, 1]\n",
    "    \n",
    "    # M√©tricas\n",
    "    print(f\"\\nAccuracy:  {accuracy_score(y_test, y_pred_dt):.4f}\")\n",
    "    print(f\"Precision: {precision_score(y_test, y_pred_dt, zero_division=0):.4f}\")\n",
    "    print(f\"Recall:    {recall_score(y_test, y_pred_dt, zero_division=0):.4f}\")\n",
    "    print(f\"F1-Score:  {f1_score(y_test, y_pred_dt, zero_division=0):.4f}\")\n",
    "    \n",
    "    if len(np.unique(y_test)) > 1:\n",
    "        print(f\"AUC-ROC:   {roc_auc_score(y_test, y_pred_proba_dt):.4f}\")\n",
    "    \n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred_dt, \n",
    "                                target_names=['Sin Falla', 'Con Falla'],\n",
    "                                zero_division=0))\n",
    "    \n",
    "    # Matriz de confusi√≥n\n",
    "    cm = confusion_matrix(y_test, y_pred_dt)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=['Sin Falla', 'Con Falla'],\n",
    "                yticklabels=['Sin Falla', 'Con Falla'])\n",
    "    plt.title('Matriz de Confusi√≥n - √Årbol de Decisi√≥n')\n",
    "    plt.ylabel('Verdadero')\n",
    "    plt.xlabel('Predicho')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\n‚úÖ √Årbol de Decisi√≥n entrenado correctamente\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Entrenamiento del Modelo LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'X_train' in locals():\n",
    "    print(\"=\"*60)\n",
    "    print(\"ENTRENANDO LIGHTGBM\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Calcular peso para datos desbalanceados\n",
    "    n_neg = np.sum(y_train == 0)\n",
    "    n_pos = np.sum(y_train == 1)\n",
    "    scale_pos_weight = n_neg / n_pos if n_pos > 0 else 1\n",
    "    \n",
    "    print(f\"Scale pos weight: {scale_pos_weight:.2f}\")\n",
    "    \n",
    "    # Crear modelo\n",
    "    lgbm_model = LGBMClassifier(\n",
    "        objective='binary',\n",
    "        metric='binary_logloss',\n",
    "        boosting_type='gbdt',\n",
    "        num_leaves=31,\n",
    "        learning_rate=0.05,\n",
    "        n_estimators=100,\n",
    "        scale_pos_weight=scale_pos_weight,\n",
    "        random_state=42,\n",
    "        verbose=-1\n",
    "    )\n",
    "    \n",
    "    # Entrenar con early stopping\n",
    "    lgbm_model.fit(\n",
    "        X_train_scaled_df, y_train,\n",
    "        eval_set=[(X_test_scaled_df, y_test)],\n",
    "        eval_names=['valid'],\n",
    "        callbacks=[\n",
    "            lgbm_model.early_stopping(stopping_rounds=20, verbose=False)\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # Predicciones\n",
    "    y_pred_lgbm = lgbm_model.predict(X_test_scaled_df)\n",
    "    y_pred_proba_lgbm = lgbm_model.predict_proba(X_test_scaled_df)[:, 1]\n",
    "    \n",
    "    # M√©tricas\n",
    "    print(f\"\\nAccuracy:  {accuracy_score(y_test, y_pred_lgbm):.4f}\")\n",
    "    print(f\"Precision: {precision_score(y_test, y_pred_lgbm, zero_division=0):.4f}\")\n",
    "    print(f\"Recall:    {recall_score(y_test, y_pred_lgbm, zero_division=0):.4f}\")\n",
    "    print(f\"F1-Score:  {f1_score(y_test, y_pred_lgbm, zero_division=0):.4f}\")\n",
    "    \n",
    "    if len(np.unique(y_test)) > 1:\n",
    "        print(f\"AUC-ROC:   {roc_auc_score(y_test, y_pred_proba_lgbm):.4f}\")\n",
    "    \n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred_lgbm, \n",
    "                                target_names=['Sin Falla', 'Con Falla'],\n",
    "                                zero_division=0))\n",
    "    \n",
    "    # Matriz de confusi√≥n\n",
    "    cm = confusion_matrix(y_test, y_pred_lgbm)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Greens',\n",
    "                xticklabels=['Sin Falla', 'Con Falla'],\n",
    "                yticklabels=['Sin Falla', 'Con Falla'])\n",
    "    plt.title('Matriz de Confusi√≥n - LightGBM')\n",
    "    plt.ylabel('Verdadero')\n",
    "    plt.xlabel('Predicho')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\n‚úÖ LightGBM entrenado correctamente\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Importancia de Caracter√≠sticas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'lgbm_model' in locals():\n",
    "    # Importancia de caracter√≠sticas LightGBM\n",
    "    lgbm_importance = pd.DataFrame({\n",
    "        'feature': X_train.columns,\n",
    "        'importance': lgbm_model.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    # Importancia de caracter√≠sticas Decision Tree\n",
    "    dt_importance = pd.DataFrame({\n",
    "        'feature': X_train.columns,\n",
    "        'importance': dt_model.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    # Visualizaci√≥n\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 8))\n",
    "    \n",
    "    # Decision Tree\n",
    "    sns.barplot(data=dt_importance.head(15), x='importance', y='feature', ax=axes[0], palette='Blues_r')\n",
    "    axes[0].set_title('Top 15 Caracter√≠sticas - √Årbol de Decisi√≥n')\n",
    "    axes[0].set_xlabel('Importancia')\n",
    "    \n",
    "    # LightGBM\n",
    "    sns.barplot(data=lgbm_importance.head(15), x='importance', y='feature', ax=axes[1], palette='Greens_r')\n",
    "    axes[1].set_title('Top 15 Caracter√≠sticas - LightGBM')\n",
    "    axes[1].set_xlabel('Importancia')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nTop 10 caracter√≠sticas m√°s importantes (LightGBM):\")\n",
    "    print(lgbm_importance.head(10).to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Comparaci√≥n de Modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'y_pred_dt' in locals() and 'y_pred_lgbm' in locals():\n",
    "    # Comparar modelos\n",
    "    resultados = []\n",
    "    \n",
    "    for nombre, (y_pred, y_proba) in [('√Årbol de Decisi√≥n', (y_pred_dt, y_pred_proba_dt)),\n",
    "                                       ('LightGBM', (y_pred_lgbm, y_pred_proba_lgbm))]:\n",
    "        result = {\n",
    "            'Modelo': nombre,\n",
    "            'Accuracy': accuracy_score(y_test, y_pred),\n",
    "            'Precision': precision_score(y_test, y_pred, zero_division=0),\n",
    "            'Recall': recall_score(y_test, y_pred, zero_division=0),\n",
    "            'F1-Score': f1_score(y_test, y_pred, zero_division=0)\n",
    "        }\n",
    "        if len(np.unique(y_test)) > 1:\n",
    "            result['AUC-ROC'] = roc_auc_score(y_test, y_proba)\n",
    "        resultados.append(result)\n",
    "    \n",
    "    df_resultados = pd.DataFrame(resultados)\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    print(\"COMPARACI√ìN DE MODELOS\")\n",
    "    print(\"=\"*70)\n",
    "    print(df_resultados.round(4).to_string(index=False))\n",
    "    \n",
    "    # Visualizaci√≥n\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score']\n",
    "    if 'AUC-ROC' in df_resultados.columns:\n",
    "        metrics.append('AUC-ROC')\n",
    "    df_resultados.set_index('Modelo')[metrics].plot(kind='bar', ax=ax)\n",
    "    plt.title('Comparaci√≥n de M√©tricas entre Modelos')\n",
    "    plt.ylabel('Score')\n",
    "    plt.xticks(rotation=0)\n",
    "    plt.legend(title='M√©tricas', loc='lower right')\n",
    "    plt.ylim(0, 1.1)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Curvas ROC\n",
    "    if len(np.unique(y_test)) > 1:\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        fpr_dt, tpr_dt, _ = roc_curve(y_test, y_pred_proba_dt)\n",
    "        fpr_lgbm, tpr_lgbm, _ = roc_curve(y_test, y_pred_proba_lgbm)\n",
    "        \n",
    "        plt.plot(fpr_dt, tpr_dt, label=f'√Årbol de Decisi√≥n (AUC={roc_auc_score(y_test, y_pred_proba_dt):.3f})')\n",
    "        plt.plot(fpr_lgbm, tpr_lgbm, label=f'LightGBM (AUC={roc_auc_score(y_test, y_pred_proba_lgbm):.3f})')\n",
    "        plt.plot([0, 1], [0, 1], 'k--', label='Random')\n",
    "        \n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.title('Curvas ROC')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Guardar Modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'dt_model' in locals() and 'lgbm_model' in locals():\n",
    "    # Guardar modelos\n",
    "    joblib.dump(dt_model, 'decision_tree_failure_model.pkl')\n",
    "    joblib.dump(lgbm_model, 'lightgbm_failure_model.pkl')\n",
    "    joblib.dump(scaler, 'feature_scaler.pkl')\n",
    "    \n",
    "    # Guardar informaci√≥n de caracter√≠sticas\n",
    "    import json\n",
    "    feature_info = {\n",
    "        'feature_names': list(X_train.columns),\n",
    "        'num_features': len(X_train.columns)\n",
    "    }\n",
    "    with open('model_feature_info.json', 'w') as f:\n",
    "        json.dump(feature_info, f, indent=2)\n",
    "    \n",
    "    print(\"‚úÖ Modelos guardados:\")\n",
    "    print(\"  - decision_tree_failure_model.pkl\")\n",
    "    print(\"  - lightgbm_failure_model.pkl\")\n",
    "    print(\"  - feature_scaler.pkl\")\n",
    "    print(\"  - model_feature_info.json\")\n",
    "    \n",
    "    # Seleccionar mejor modelo\n",
    "    dt_f1 = f1_score(y_test, y_pred_dt, zero_division=0)\n",
    "    lgbm_f1 = f1_score(y_test, y_pred_lgbm, zero_division=0)\n",
    "    \n",
    "    if lgbm_f1 >= dt_f1:\n",
    "        print(f\"\\nüèÜ Mejor modelo: LightGBM (F1-Score: {lgbm_f1:.4f})\")\n",
    "    else:\n",
    "        print(f\"\\nüèÜ Mejor modelo: √Årbol de Decisi√≥n (F1-Score: {dt_f1:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Funci√≥n para Predicciones en Nuevos Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predecir_fallas(nuevos_datos, modelo, scaler, feature_names):\n",
    "    \"\"\"\n",
    "    Hacer predicciones de fallas en nuevos datos\n",
    "    \n",
    "    Par√°metros:\n",
    "    - nuevos_datos: DataFrame con las caracter√≠sticas\n",
    "    - modelo: Modelo entrenado\n",
    "    - scaler: Escalador ajustado\n",
    "    - feature_names: Lista de nombres de caracter√≠sticas\n",
    "    \n",
    "    Retorna:\n",
    "    - predicciones: Array con las clases predichas\n",
    "    - probabilidades: Array con las probabilidades de falla\n",
    "    \"\"\"\n",
    "    # Preparar datos\n",
    "    datos_preparados = nuevos_datos[feature_names].copy()\n",
    "    datos_preparados = datos_preparados.fillna(0).replace([np.inf, -np.inf], 0)\n",
    "    \n",
    "    # Escalar\n",
    "    datos_scaled = scaler.transform(datos_preparados)\n",
    "    datos_scaled_df = pd.DataFrame(datos_scaled, columns=feature_names)\n",
    "    \n",
    "    # Predecir\n",
    "    predicciones = modelo.predict(datos_scaled_df)\n",
    "    probabilidades = modelo.predict_proba(datos_scaled_df)[:, 1]\n",
    "    \n",
    "    return predicciones, probabilidades\n",
    "\n",
    "# Ejemplo de uso\n",
    "if 'lgbm_model' in locals() and 'X_test' in locals():\n",
    "    print(\"=\" * 60)\n",
    "    print(\"EJEMPLO DE PREDICCI√ìN EN NUEVOS DATOS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    ejemplo_datos = X_test.head(5)\n",
    "    pred, prob = predecir_fallas(ejemplo_datos, lgbm_model, scaler, list(X_train.columns))\n",
    "    \n",
    "    for i in range(len(pred)):\n",
    "        estado = \"CON FALLA PR√ìXIMA\" if pred[i] == 1 else \"Sin falla pr√≥xima\"\n",
    "        print(f\"\\nMuestra {i+1}: {estado}\")\n",
    "        print(f\"  Probabilidad de falla: {prob[i]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resumen\n",
    "\n",
    "En este notebook hemos:\n",
    "\n",
    "1. ‚úÖ Cargado datos de la tabla `faliure_probability_base` (granularidad diaria)\n",
    "2. ‚úÖ Analizado la variable objetivo `faliure` (falla en pr√≥ximos 7 d√≠as)\n",
    "3. ‚úÖ Preparado caracter√≠sticas para entrenamiento\n",
    "4. ‚úÖ Entrenado modelo **√Årbol de Decisi√≥n**\n",
    "5. ‚úÖ Entrenado modelo **LightGBM**\n",
    "6. ‚úÖ Comparado m√©tricas de ambos modelos\n",
    "7. ‚úÖ Visualizado importancia de caracter√≠sticas\n",
    "8. ‚úÖ Guardado modelos para uso en producci√≥n\n",
    "9. ‚úÖ Creado funci√≥n para predicciones en nuevos datos\n",
    "\n",
    "**Los modelos est√°n listos para predecir fallas en activos de mantenimiento.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
