{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Métricas de Evaluación de Modelos de Clasificación\n",
    "\n",
    "Este notebook cubre:\n",
    "- Métricas de evaluación para modelos de clasificación binaria\n",
    "- Análisis detallado de las métricas para predicción de fallas\n",
    "- Visualización de métricas\n",
    "- Comparación entre Árbol de Decisión y LightGBM\n",
    "\n",
    "**Modelos evaluados:**\n",
    "- Árbol de Decisión (Decision Tree)\n",
    "- LightGBM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Importación de Librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    confusion_matrix, classification_report,\n",
    "    roc_curve, auc, roc_auc_score,\n",
    "    precision_recall_curve, average_precision_score\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "import mysql.connector\n",
    "from mysql.connector import Error\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"Librerías importadas correctamente\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Cargar Datos y Entrenar Modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuración de la base de datos\n",
    "DB_CONFIG = {\n",
    "    'host': '127.0.0.1',\n",
    "    'database': 'palantir_maintenance',\n",
    "    'user': 'root',\n",
    "    'password': 'admin',\n",
    "    'port': 3306\n",
    "}\n",
    "\n",
    "def cargar_y_preparar_datos():\n",
    "    \"\"\"Cargar datos y preparar para evaluación\"\"\"\n",
    "    try:\n",
    "        connection = mysql.connector.connect(**DB_CONFIG)\n",
    "        if connection.is_connected():\n",
    "            query = \"SELECT * FROM faliure_probability_base ORDER BY asset_id, reading_date\"\n",
    "            df = pd.read_sql(query, connection)\n",
    "            connection.close()\n",
    "            \n",
    "            # Preparar datos\n",
    "            exclude_cols = ['base_id', 'asset_id', 'reading_date', 'faliure', \n",
    "                           'asset_status', 'created_at', 'updated_at']\n",
    "            feature_columns = [col for col in df.columns if col not in exclude_cols]\n",
    "            X = df[feature_columns].select_dtypes(include=[np.number]).fillna(0)\n",
    "            y = df['faliure'].astype(int)\n",
    "            \n",
    "            return X, y\n",
    "    except Error as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return None, None\n",
    "\n",
    "X, y = cargar_y_preparar_datos()\n",
    "\n",
    "if X is not None and len(np.unique(y)) > 1:\n",
    "    # Dividir datos\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "    \n",
    "    # Escalar\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    X_train_scaled_df = pd.DataFrame(X_train_scaled, columns=X_train.columns)\n",
    "    X_test_scaled_df = pd.DataFrame(X_test_scaled, columns=X_test.columns)\n",
    "    \n",
    "    # Entrenar Decision Tree\n",
    "    dt_model = DecisionTreeClassifier(max_depth=10, min_samples_split=5, min_samples_leaf=2, \n",
    "                                       class_weight='balanced', random_state=42)\n",
    "    dt_model.fit(X_train_scaled, y_train)\n",
    "    y_pred_dt = dt_model.predict(X_test_scaled)\n",
    "    y_pred_proba_dt = dt_model.predict_proba(X_test_scaled)[:, 1]\n",
    "    \n",
    "    # Entrenar LightGBM\n",
    "    n_neg = np.sum(y_train == 0)\n",
    "    n_pos = np.sum(y_train == 1)\n",
    "    scale_pos_weight = n_neg / n_pos if n_pos > 0 else 1\n",
    "    \n",
    "    lgbm_model = LGBMClassifier(objective='binary', num_leaves=31, learning_rate=0.05,\n",
    "                                 n_estimators=100, scale_pos_weight=scale_pos_weight,\n",
    "                                 random_state=42, verbose=-1)\n",
    "    lgbm_model.fit(X_train_scaled_df, y_train)\n",
    "    y_pred_lgbm = lgbm_model.predict(X_test_scaled_df)\n",
    "    y_pred_proba_lgbm = lgbm_model.predict_proba(X_test_scaled_df)[:, 1]\n",
    "    \n",
    "    print(f\"Datos cargados: {X.shape}\")\n",
    "    print(f\"Modelos entrenados correctamente\")\n",
    "else:\n",
    "    print(\"Error al cargar datos o solo hay una clase\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Métricas Básicas de Clasificación\n",
    "\n",
    "### 3.1 Explicación de las Métricas\n",
    "\n",
    "- **Accuracy:** Proporción de predicciones correctas sobre el total. (TP + TN) / (TP + TN + FP + FN)\n",
    "- **Precision:** Proporción de predicciones positivas que son correctas. TP / (TP + FP)\n",
    "- **Recall:** Proporción de casos positivos correctamente identificados. TP / (TP + FN)\n",
    "- **F1-Score:** Media armónica de Precision y Recall. 2 * (Precision * Recall) / (Precision + Recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcular_metricas(y_true, y_pred, y_pred_proba, nombre_modelo):\n",
    "    \"\"\"Calcular y mostrar métricas de clasificación\"\"\"\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred, zero_division=0)\n",
    "    recall = recall_score(y_true, y_pred, zero_division=0)\n",
    "    f1 = f1_score(y_true, y_pred, zero_division=0)\n",
    "    \n",
    "    metricas = {\n",
    "        'Modelo': nombre_modelo,\n",
    "        'Accuracy': accuracy,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1-Score': f1\n",
    "    }\n",
    "    \n",
    "    if len(np.unique(y_true)) > 1:\n",
    "        metricas['AUC-ROC'] = roc_auc_score(y_true, y_pred_proba)\n",
    "        metricas['Avg Precision'] = average_precision_score(y_true, y_pred_proba)\n",
    "    \n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Métricas para {nombre_modelo}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    print(f\"Accuracy:      {accuracy:.4f}\")\n",
    "    print(f\"Precision:     {precision:.4f}\")\n",
    "    print(f\"Recall:        {recall:.4f}\")\n",
    "    print(f\"F1-Score:      {f1:.4f}\")\n",
    "    if 'AUC-ROC' in metricas:\n",
    "        print(f\"AUC-ROC:       {metricas['AUC-ROC']:.4f}\")\n",
    "        print(f\"Avg Precision: {metricas['Avg Precision']:.4f}\")\n",
    "    \n",
    "    return metricas\n",
    "\n",
    "if 'y_test' in locals():\n",
    "    metricas_dt = calcular_metricas(y_test, y_pred_dt, y_pred_proba_dt, \"Árbol de Decisión\")\n",
    "    metricas_lgbm = calcular_metricas(y_test, y_pred_lgbm, y_pred_proba_lgbm, \"LightGBM\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Matriz de Confusión Detallada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix_detallada(y_true, y_pred, nombre_modelo):\n",
    "    \"\"\"Visualizar matriz de confusión con métricas adicionales\"\"\"\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Matriz absoluta\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[0],\n",
    "                xticklabels=['Sin Falla', 'Con Falla'],\n",
    "                yticklabels=['Sin Falla', 'Con Falla'])\n",
    "    axes[0].set_title(f'Matriz de Confusión - {nombre_modelo} (Absoluta)')\n",
    "    axes[0].set_ylabel('Verdadero')\n",
    "    axes[0].set_xlabel('Predicho')\n",
    "    \n",
    "    # Matriz normalizada\n",
    "    sns.heatmap(cm_normalized, annot=True, fmt='.2%', cmap='Blues', ax=axes[1],\n",
    "                xticklabels=['Sin Falla', 'Con Falla'],\n",
    "                yticklabels=['Sin Falla', 'Con Falla'])\n",
    "    axes[1].set_title(f'Matriz de Confusión - {nombre_modelo} (Normalizada)')\n",
    "    axes[1].set_ylabel('Verdadero')\n",
    "    axes[1].set_xlabel('Predicho')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Calcular métricas detalladas\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    print(f\"\\nDesglose para {nombre_modelo}:\")\n",
    "    print(f\"  Verdaderos Negativos (TN): {tn}\")\n",
    "    print(f\"  Falsos Positivos (FP):     {fp}\")\n",
    "    print(f\"  Falsos Negativos (FN):     {fn}\")\n",
    "    print(f\"  Verdaderos Positivos (TP): {tp}\")\n",
    "\n",
    "if 'y_test' in locals():\n",
    "    plot_confusion_matrix_detallada(y_test, y_pred_dt, \"Árbol de Decisión\")\n",
    "    plot_confusion_matrix_detallada(y_test, y_pred_lgbm, \"LightGBM\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Classification Report Detallado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'y_test' in locals():\n",
    "    print(\"=\"*60)\n",
    "    print(\"Classification Report - Árbol de Decisión\")\n",
    "    print(\"=\"*60)\n",
    "    print(classification_report(y_test, y_pred_dt, \n",
    "                                target_names=['Sin Falla', 'Con Falla'],\n",
    "                                zero_division=0))\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Classification Report - LightGBM\")\n",
    "    print(\"=\"*60)\n",
    "    print(classification_report(y_test, y_pred_lgbm, \n",
    "                                target_names=['Sin Falla', 'Con Falla'],\n",
    "                                zero_division=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Curvas ROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'y_test' in locals() and len(np.unique(y_test)) > 1:\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    \n",
    "    # Árbol de Decisión\n",
    "    fpr_dt, tpr_dt, _ = roc_curve(y_test, y_pred_proba_dt)\n",
    "    auc_dt = roc_auc_score(y_test, y_pred_proba_dt)\n",
    "    plt.plot(fpr_dt, tpr_dt, lw=2, label=f'Árbol de Decisión (AUC = {auc_dt:.3f})')\n",
    "    \n",
    "    # LightGBM\n",
    "    fpr_lgbm, tpr_lgbm, _ = roc_curve(y_test, y_pred_proba_lgbm)\n",
    "    auc_lgbm = roc_auc_score(y_test, y_pred_proba_lgbm)\n",
    "    plt.plot(fpr_lgbm, tpr_lgbm, lw=2, label=f'LightGBM (AUC = {auc_lgbm:.3f})')\n",
    "    \n",
    "    # Línea diagonal (random)\n",
    "    plt.plot([0, 1], [0, 1], 'k--', lw=2, label='Random (AUC = 0.500)')\n",
    "    \n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate', fontsize=12)\n",
    "    plt.ylabel('True Positive Rate', fontsize=12)\n",
    "    plt.title('Curvas ROC - Comparación de Modelos', fontsize=14)\n",
    "    plt.legend(loc=\"lower right\", fontsize=11)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No hay suficientes clases para calcular curvas ROC\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Curvas Precision-Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'y_test' in locals() and len(np.unique(y_test)) > 1:\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    \n",
    "    # Árbol de Decisión\n",
    "    precision_dt, recall_dt, _ = precision_recall_curve(y_test, y_pred_proba_dt)\n",
    "    ap_dt = average_precision_score(y_test, y_pred_proba_dt)\n",
    "    plt.plot(recall_dt, precision_dt, lw=2, label=f'Árbol de Decisión (AP = {ap_dt:.3f})')\n",
    "    \n",
    "    # LightGBM\n",
    "    precision_lgbm, recall_lgbm, _ = precision_recall_curve(y_test, y_pred_proba_lgbm)\n",
    "    ap_lgbm = average_precision_score(y_test, y_pred_proba_lgbm)\n",
    "    plt.plot(recall_lgbm, precision_lgbm, lw=2, label=f'LightGBM (AP = {ap_lgbm:.3f})')\n",
    "    \n",
    "    plt.xlabel('Recall', fontsize=12)\n",
    "    plt.ylabel('Precision', fontsize=12)\n",
    "    plt.title('Curvas Precision-Recall - Comparación de Modelos', fontsize=14)\n",
    "    plt.legend(loc=\"lower left\", fontsize=11)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No hay suficientes clases para calcular curvas PR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Comparación Completa de Métricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'metricas_dt' in locals() and 'metricas_lgbm' in locals():\n",
    "    df_comparacion = pd.DataFrame([metricas_dt, metricas_lgbm])\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"COMPARACIÓN COMPLETA DE MODELOS\")\n",
    "    print(\"=\"*70)\n",
    "    print(df_comparacion.round(4).to_string(index=False))\n",
    "    \n",
    "    # Visualización\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    metrics_to_plot = ['Accuracy', 'Precision', 'Recall', 'F1-Score']\n",
    "    if 'AUC-ROC' in df_comparacion.columns:\n",
    "        metrics_to_plot.append('AUC-ROC')\n",
    "    \n",
    "    df_comparacion.set_index('Modelo')[metrics_to_plot].plot(kind='bar', ax=ax)\n",
    "    plt.title('Comparación de Métricas entre Modelos', fontsize=14)\n",
    "    plt.ylabel('Score')\n",
    "    plt.xticks(rotation=0)\n",
    "    plt.legend(title='Métricas', loc='lower right')\n",
    "    plt.ylim(0, 1.1)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Mejor modelo\n",
    "    best_model = df_comparacion.loc[df_comparacion['F1-Score'].idxmax(), 'Modelo']\n",
    "    print(f\"\\n✓ Mejor modelo (basado en F1-Score): {best_model}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resumen\n",
    "\n",
    "En este notebook hemos aprendido:\n",
    "1. ✅ Métricas básicas de clasificación (Accuracy, Precision, Recall, F1-Score)\n",
    "2. ✅ Análisis detallado de matrices de confusión\n",
    "3. ✅ Classification reports por clase\n",
    "4. ✅ Curvas ROC para clasificación binaria\n",
    "5. ✅ Curvas Precision-Recall\n",
    "6. ✅ Comparación entre Árbol de Decisión y LightGBM"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
