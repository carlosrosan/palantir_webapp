{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Métricas de Evaluación de Modelos de Clasificación\n",
    "\n",
    "Este notebook cubre:\n",
    "- Métricas de evaluación para modelos de clasificación\n",
    "- Análisis detallado de las métricas generadas por los modelos\n",
    "- Visualización de métricas\n",
    "- Interpretación de resultados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Importación de Librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Librerías importadas correctamente\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    confusion_matrix, classification_report,\n",
    "    roc_curve, auc, roc_auc_score,\n",
    "    precision_recall_curve, average_precision_score\n",
    ")\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from itertools import cycle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"Librerías importadas correctamente\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Cargar Modelos y Predicciones del Notebook Anterior\n",
    "\n",
    "**Nota:** Ejecuta primero el notebook `02_modelos_clasificacion.ipynb` para tener los modelos entrenados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Asegúrate de haber ejecutado el notebook anterior o descomenta el código de arriba\n"
     ]
    }
   ],
   "source": [
    "# Si ejecutaste el notebook anterior, estos objetos deberían estar disponibles\n",
    "# Si no, descomenta y ejecuta el código siguiente para entrenar modelos rápidamente\n",
    "\n",
    "# Código de ejemplo para cargar datos y entrenar un modelo rápido\n",
    "# (Descomenta si no ejecutaste el notebook anterior)\n",
    "\"\"\"\n",
    "import mysql.connector\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "DB_CONFIG = {\n",
    "    'host': '127.0.0.1',\n",
    "    'database': 'palantir_maintenance',\n",
    "    'user': 'root',\n",
    "    'password': 'admin',\n",
    "    'port': 3306\n",
    "}\n",
    "\n",
    "connection = mysql.connector.connect(**DB_CONFIG)\n",
    "df = pd.read_sql(\"SELECT * FROM faliure_probability_base\", connection)\n",
    "connection.close()\n",
    "\n",
    "def crear_clase_riesgo(row):\n",
    "    failure_count = row.get('failure_count_365d', 0) or 0\n",
    "    sensor_critical = row.get('sensor_critical_count_30d', 0) or 0\n",
    "    if failure_count >= 5 or sensor_critical >= 10:\n",
    "        return 3\n",
    "    elif failure_count >= 3 or sensor_critical >= 5:\n",
    "        return 2\n",
    "    elif failure_count >= 1 or sensor_critical >= 2:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "df['clase_riesgo'] = df.apply(crear_clase_riesgo, axis=1)\n",
    "\n",
    "feature_columns = [\n",
    "    'asset_age_days', 'sensor_total_readings_30d', 'sensor_warning_count_30d',\n",
    "    'sensor_critical_count_30d', 'failure_count_365d', 'failure_critical_count',\n",
    "    'task_total_365d', 'order_total_365d'\n",
    "]\n",
    "\n",
    "available_features = [col for col in feature_columns if col in df.columns]\n",
    "X = df[available_features].fillna(0)\n",
    "y = df['clase_riesgo']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "dt_model = DecisionTreeClassifier(max_depth=5, random_state=42)\n",
    "dt_model.fit(X_train, y_train)\n",
    "y_pred_dt = dt_model.predict(X_test)\n",
    "\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "y_pred_rf = rf_model.predict(X_test)\n",
    "\"\"\"\n",
    "\n",
    "print(\"Asegúrate de haber ejecutado el notebook anterior o descomenta el código de arriba\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Métricas Básicas de Clasificación\n",
    "\n",
    "### 3.1 Accuracy, Precision, Recall, F1-Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ejecuta primero el notebook de modelos de clasificación\n"
     ]
    }
   ],
   "source": [
    "def calcular_metricas_basicas(y_true, y_pred, nombre_modelo=\"Modelo\"):\n",
    "    \"\"\"Calcular métricas básicas de clasificación\"\"\"\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "    recall = recall_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "    f1 = f1_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "    \n",
    "    metricas = {\n",
    "        'Modelo': nombre_modelo,\n",
    "        'Accuracy': accuracy,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1-Score': f1\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n=== Métricas para {nombre_modelo} ===\")\n",
    "    print(f\"Accuracy:  {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall:    {recall:.4f}\")\n",
    "    print(f\"F1-Score:  {f1:.4f}\")\n",
    "    \n",
    "    return metricas\n",
    "\n",
    "# Calcular métricas para los modelos (si están disponibles)\n",
    "if 'y_test' in locals() and 'y_pred_dt' in locals():\n",
    "    metricas_dt = calcular_metricas_basicas(y_test, y_pred_dt, \"Árbol de Decisión\")\n",
    "    \n",
    "if 'y_test' in locals() and 'y_pred_rf' in locals():\n",
    "    metricas_rf = calcular_metricas_basicas(y_test, y_pred_rf, \"Random Forest\")\n",
    "else:\n",
    "    print(\"Ejecuta primero el notebook de modelos de clasificación\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Explicación de las Métricas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Accuracy (Precisión):** Proporción de predicciones correctas sobre el total.\n",
    "- Fórmula: (TP + TN) / (TP + TN + FP + FN)\n",
    "- Útil cuando las clases están balanceadas\n",
    "\n",
    "**Precision (Precisión por clase):** Proporción de predicciones positivas que son correctas.\n",
    "- Fórmula: TP / (TP + FP)\n",
    "- Mide cuántas de las predicciones positivas son realmente positivas\n",
    "\n",
    "**Recall (Sensibilidad):** Proporción de casos positivos que fueron correctamente identificados.\n",
    "- Fórmula: TP / (TP + FN)\n",
    "- Mide cuántos casos positivos fueron capturados\n",
    "\n",
    "**F1-Score:** Media armónica de Precision y Recall.\n",
    "- Fórmula: 2 * (Precision * Recall) / (Precision + Recall)\n",
    "- Balance entre Precision y Recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Matriz de Confusión Detallada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix_detallada(y_true, y_pred, nombre_modelo, clases_nombres):\n",
    "    \"\"\"Visualizar matriz de confusión con métricas adicionales\"\"\"\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    \n",
    "    # Matriz de confusión normalizada\n",
    "    cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    \n",
    "    # Gráfico 1: Matriz absoluta\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[0],\n",
    "                xticklabels=clases_nombres, yticklabels=clases_nombres)\n",
    "    axes[0].set_title(f'Matriz de Confusión - {nombre_modelo} (Absoluta)')\n",
    "    axes[0].set_ylabel('Verdadero')\n",
    "    axes[0].set_xlabel('Predicho')\n",
    "    \n",
    "    # Gráfico 2: Matriz normalizada\n",
    "    sns.heatmap(cm_normalized, annot=True, fmt='.2%', cmap='Blues', ax=axes[1],\n",
    "                xticklabels=clases_nombres, yticklabels=clases_nombres)\n",
    "    axes[1].set_title(f'Matriz de Confusión - {nombre_modelo} (Normalizada)')\n",
    "    axes[1].set_ylabel('Verdadero')\n",
    "    axes[1].set_xlabel('Predicho')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Calcular métricas por clase\n",
    "    print(f\"\\nMétricas por clase para {nombre_modelo}:\")\n",
    "    for i, clase_nombre in enumerate(clases_nombres):\n",
    "        tp = cm[i, i]\n",
    "        fp = cm[:, i].sum() - tp\n",
    "        fn = cm[i, :].sum() - tp\n",
    "        tn = cm.sum() - (tp + fp + fn)\n",
    "        \n",
    "        precision_clase = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "        recall_clase = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        f1_clase = 2 * (precision_clase * recall_clase) / (precision_clase + recall_clase) if (precision_clase + recall_clase) > 0 else 0\n",
    "        \n",
    "        print(f\"\\n{clase_nombre}:\")\n",
    "        print(f\"  Precision: {precision_clase:.4f}\")\n",
    "        print(f\"  Recall:    {recall_clase:.4f}\")\n",
    "        print(f\"  F1-Score:  {f1_clase:.4f}\")\n",
    "\n",
    "# Visualizar matrices de confusión\n",
    "clases_nombres = ['Bajo', 'Medio', 'Alto', 'Crítico']\n",
    "\n",
    "if 'y_test' in locals() and 'y_pred_dt' in locals():\n",
    "    plot_confusion_matrix_detallada(y_test, y_pred_dt, \"Árbol de Decisión\", clases_nombres)\n",
    "    \n",
    "if 'y_test' in locals() and 'y_pred_rf' in locals():\n",
    "    plot_confusion_matrix_detallada(y_test, y_pred_rf, \"Random Forest\", clases_nombres)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Classification Report Detallado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analizar_classification_report(y_true, y_pred, nombre_modelo, clases_nombres):\n",
    "    \"\"\"Analizar el reporte de clasificación\"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Classification Report - {nombre_modelo}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    report = classification_report(y_true, y_pred, target_names=clases_nombres, output_dict=True)\n",
    "    \n",
    "    # Convertir a DataFrame para mejor visualización\n",
    "    df_report = pd.DataFrame(report).transpose()\n",
    "    print(df_report.round(4))\n",
    "    \n",
    "    # Visualización\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    df_report_plot = df_report.iloc[:-3, :3]  # Excluir 'accuracy', 'macro avg', 'weighted avg'\n",
    "    df_report_plot.plot(kind='bar', ax=ax)\n",
    "    plt.title(f'Métricas por Clase - {nombre_modelo}')\n",
    "    plt.ylabel('Score')\n",
    "    plt.xlabel('Clase')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.legend(title='Métricas')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return df_report\n",
    "\n",
    "# Analizar reportes\n",
    "if 'y_test' in locals() and 'y_pred_dt' in locals():\n",
    "    report_dt = analizar_classification_report(y_test, y_pred_dt, \"Árbol de Decisión\", clases_nombres)\n",
    "    \n",
    "if 'y_test' in locals() and 'y_pred_rf' in locals():\n",
    "    report_rf = analizar_classification_report(y_test, y_pred_rf, \"Random Forest\", clases_nombres)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Curvas ROC (Receiver Operating Characteristic)\n",
    "\n",
    "Para problemas multiclase, usamos One-vs-Rest (OvR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc_curves_multiclass(y_true, y_pred_proba, nombre_modelo, clases_nombres):\n",
    "    \"\"\"Plot ROC curves para clasificación multiclase\"\"\"\n",
    "    # Binarizar las etiquetas\n",
    "    y_true_bin = label_binarize(y_true, classes=range(len(clases_nombres)))\n",
    "    n_classes = len(clases_nombres)\n",
    "    \n",
    "    # Calcular ROC para cada clase\n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = dict()\n",
    "    \n",
    "    for i in range(n_classes):\n",
    "        fpr[i], tpr[i], _ = roc_curve(y_true_bin[:, i], y_pred_proba[:, i])\n",
    "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "    \n",
    "    # Calcular micro-average ROC\n",
    "    fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_true_bin.ravel(), y_pred_proba.ravel())\n",
    "    roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    \n",
    "    # Plot para cada clase\n",
    "    colors = cycle(['aqua', 'darkorange', 'cornflowerblue', 'red'])\n",
    "    for i, color in zip(range(n_classes), colors):\n",
    "        plt.plot(fpr[i], tpr[i], color=color, lw=2,\n",
    "                label=f'ROC {clases_nombres[i]} (AUC = {roc_auc[i]:.2f})')\n",
    "    \n",
    "    # Plot micro-average\n",
    "    plt.plot(fpr[\"micro\"], tpr[\"micro\"], color='deeppink', linestyle='--', lw=2,\n",
    "            label=f'ROC micro-average (AUC = {roc_auc[\"micro\"]:.2f})')\n",
    "    \n",
    "    plt.plot([0, 1], [0, 1], 'k--', lw=2, label='Random')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title(f'ROC Curves - {nombre_modelo}')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return roc_auc\n",
    "\n",
    "# Obtener probabilidades de predicción (si el modelo las soporta)\n",
    "if 'dt_model' in locals() and 'X_test' in locals():\n",
    "    try:\n",
    "        y_pred_proba_dt = dt_model.predict_proba(X_test)\n",
    "        roc_auc_dt = plot_roc_curves_multiclass(y_test, y_pred_proba_dt, \"Árbol de Decisión\", clases_nombres)\n",
    "        print(f\"\\nAUC scores - Árbol de Decisión: {roc_auc_dt}\")\n",
    "    except Exception as e:\n",
    "        print(f\"No se pudieron calcular las curvas ROC: {e}\")\n",
    "        \n",
    "if 'rf_model' in locals() and 'X_test' in locals():\n",
    "    try:\n",
    "        y_pred_proba_rf = rf_model.predict_proba(X_test)\n",
    "        roc_auc_rf = plot_roc_curves_multiclass(y_test, y_pred_proba_rf, \"Random Forest\", clases_nombres)\n",
    "        print(f\"\\nAUC scores - Random Forest: {roc_auc_rf}\")\n",
    "    except Exception as e:\n",
    "        print(f\"No se pudieron calcular las curvas ROC: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Curvas Precision-Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_precision_recall_curves(y_true, y_pred_proba, nombre_modelo, clases_nombres):\n",
    "    \"\"\"Plot Precision-Recall curves para clasificación multiclase\"\"\"\n",
    "    y_true_bin = label_binarize(y_true, classes=range(len(clases_nombres)))\n",
    "    n_classes = len(clases_nombres)\n",
    "    \n",
    "    precision = dict()\n",
    "    recall = dict()\n",
    "    average_precision = dict()\n",
    "    \n",
    "    for i in range(n_classes):\n",
    "        precision[i], recall[i], _ = precision_recall_curve(y_true_bin[:, i], y_pred_proba[:, i])\n",
    "        average_precision[i] = average_precision_score(y_true_bin[:, i], y_pred_proba[:, i])\n",
    "    \n",
    "    # Micro-average\n",
    "    precision[\"micro\"], recall[\"micro\"], _ = precision_recall_curve(\n",
    "        y_true_bin.ravel(), y_pred_proba.ravel())\n",
    "    average_precision[\"micro\"] = average_precision_score(y_true_bin, y_pred_proba, average=\"micro\")\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    \n",
    "    colors = cycle(['aqua', 'darkorange', 'cornflowerblue', 'red'])\n",
    "    for i, color in zip(range(n_classes), colors):\n",
    "        plt.plot(recall[i], precision[i], color=color, lw=2,\n",
    "                label=f'PR {clases_nombres[i]} (AP = {average_precision[i]:.2f})')\n",
    "    \n",
    "    plt.plot(recall[\"micro\"], precision[\"micro\"], color='deeppink', linestyle='--', lw=2,\n",
    "            label=f'PR micro-average (AP = {average_precision[\"micro\"]:.2f})')\n",
    "    \n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.title(f'Precision-Recall Curves - {nombre_modelo}')\n",
    "    plt.legend(loc=\"lower left\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return average_precision\n",
    "\n",
    "# Plot Precision-Recall curves\n",
    "if 'y_test' in locals() and 'dt_model' in locals() and 'X_test' in locals():\n",
    "    try:\n",
    "        y_pred_proba_dt = dt_model.predict_proba(X_test)\n",
    "        ap_dt = plot_precision_recall_curves(y_test, y_pred_proba_dt, \"Árbol de Decisión\", clases_nombres)\n",
    "        print(f\"\\nAverage Precision - Árbol de Decisión: {ap_dt}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        \n",
    "if 'y_test' in locals() and 'rf_model' in locals() and 'X_test' in locals():\n",
    "    try:\n",
    "        y_pred_proba_rf = rf_model.predict_proba(X_test)\n",
    "        ap_rf = plot_precision_recall_curves(y_test, y_pred_proba_rf, \"Random Forest\", clases_nombres)\n",
    "        print(f\"\\nAverage Precision - Random Forest: {ap_rf}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Comparación Completa de Métricas entre Modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No hay suficientes modelos para comparar\n"
     ]
    }
   ],
   "source": [
    "def comparar_modelos_completo(y_test, modelos_dict, clases_nombres):\n",
    "    \"\"\"Comparar todos los modelos con todas las métricas\"\"\"\n",
    "    resultados = []\n",
    "    \n",
    "    for nombre, (y_pred, modelo) in modelos_dict.items():\n",
    "        # Métricas básicas\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        precision = precision_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "        recall = recall_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "        f1 = f1_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "        \n",
    "        # AUC (si el modelo tiene predict_proba)\n",
    "        try:\n",
    "            y_pred_proba = modelo.predict_proba(y_test.values.reshape(-1, 1) if hasattr(y_test, 'values') else y_test)\n",
    "            y_test_bin = label_binarize(y_test, classes=range(len(clases_nombres)))\n",
    "            auc_score = roc_auc_score(y_test_bin, y_pred_proba, average='weighted', multi_class='ovr')\n",
    "        except:\n",
    "            auc_score = None\n",
    "        \n",
    "        resultados.append({\n",
    "            'Modelo': nombre,\n",
    "            'Accuracy': accuracy,\n",
    "            'Precision': precision,\n",
    "            'Recall': recall,\n",
    "            'F1-Score': f1,\n",
    "            'AUC': auc_score\n",
    "        })\n",
    "    \n",
    "    df_comparacion = pd.DataFrame(resultados)\n",
    "    \n",
    "    # Visualización\n",
    "    fig, axes = plt.subplots(2, 1, figsize=(12, 10))\n",
    "    \n",
    "    # Gráfico 1: Métricas básicas\n",
    "    df_comparacion_plot = df_comparacion.set_index('Modelo')[['Accuracy', 'Precision', 'Recall', 'F1-Score']]\n",
    "    df_comparacion_plot.plot(kind='bar', ax=axes[0])\n",
    "    axes[0].set_title('Comparación de Métricas Básicas')\n",
    "    axes[0].set_ylabel('Score')\n",
    "    axes[0].legend(title='Métricas')\n",
    "    axes[0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Gráfico 2: Tabla de resultados\n",
    "    axes[1].axis('tight')\n",
    "    axes[1].axis('off')\n",
    "    tabla = axes[1].table(cellText=df_comparacion.round(4).values,\n",
    "                         colLabels=df_comparacion.columns,\n",
    "                         cellLoc='center',\n",
    "                         loc='center')\n",
    "    tabla.auto_set_font_size(False)\n",
    "    tabla.set_fontsize(10)\n",
    "    tabla.scale(1.2, 1.5)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return df_comparacion\n",
    "\n",
    "# Comparar modelos\n",
    "if 'y_test' in locals() and 'y_pred_dt' in locals() and 'y_pred_rf' in locals():\n",
    "    modelos_dict = {\n",
    "        'Árbol de Decisión': (y_pred_dt, dt_model if 'dt_model' in locals() else None),\n",
    "        'Random Forest': (y_pred_rf, rf_model if 'rf_model' in locals() else None)\n",
    "    }\n",
    "    \n",
    "    # Filtrar modelos válidos\n",
    "    modelos_dict = {k: v for k, v in modelos_dict.items() if v[1] is not None}\n",
    "    \n",
    "    if modelos_dict:\n",
    "        df_comparacion = comparar_modelos_completo(y_test, modelos_dict, clases_nombres)\n",
    "        print(\"\\nComparación de Modelos:\")\n",
    "        print(df_comparacion.round(4))\n",
    "else:\n",
    "    print(\"No hay suficientes modelos para comparar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resumen\n",
    "\n",
    "En este notebook hemos aprendido:\n",
    "1. ✅ Métricas básicas de clasificación (Accuracy, Precision, Recall, F1-Score)\n",
    "2. ✅ Análisis detallado de matrices de confusión\n",
    "3. ✅ Classification reports por clase\n",
    "4. ✅ Curvas ROC para problemas multiclase\n",
    "5. ✅ Curvas Precision-Recall\n",
    "6. ✅ Comparación completa de modelos\n",
    "\n",
    "**Próximos pasos:** En el siguiente notebook aprenderemos sobre LightGBM."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "palantir_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
