{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introducción a LightGBM\n",
    "\n",
    "Este notebook cubre:\n",
    "- ¿Qué es LightGBM?\n",
    "- Ventajas y características\n",
    "- Instalación y configuración\n",
    "- Uso básico de LightGBM\n",
    "- Parámetros principales\n",
    "- Comparación con otros algoritmos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. ¿Qué es LightGBM?\n",
    "\n",
    "**LightGBM** (Light Gradient Boosting Machine) es un framework de gradient boosting desarrollado por Microsoft.\n",
    "\n",
    "### Características principales:\n",
    "- **Rápido**: Optimizado para velocidad y eficiencia de memoria\n",
    "- **Escalable**: Maneja grandes volúmenes de datos\n",
    "- **Preciso**: Alto rendimiento en tareas de machine learning\n",
    "- **Soporte nativo**: Maneja valores categóricos sin necesidad de one-hot encoding\n",
    "- **Múltiples tareas**: Clasificación, regresión, ranking\n",
    "\n",
    "### Ventajas sobre otros algoritmos:\n",
    "1. **Velocidad**: Más rápido que XGBoost y otros algoritmos de boosting\n",
    "2. **Memoria**: Uso eficiente de memoria\n",
    "3. **Precisión**: Mejor rendimiento en muchos casos\n",
    "4. **Paralelización**: Soporte para GPU y procesamiento paralelo\n",
    "5. **Manejo de datos grandes**: Optimizado para datasets grandes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Instalación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalar LightGBM (descomenta si no está instalado)\n",
    "# !pip install lightgbm\n",
    "\n",
    "import lightgbm as lgb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-v0_8')\n",
    "\n",
    "print(f\"LightGBM versión: {lgb.__version__}\")\n",
    "print(\"Librerías importadas correctamente\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Conceptos Fundamentales de LightGBM\n",
    "\n",
    "### 3.1 Gradient Boosting\n",
    "\n",
    "LightGBM utiliza **Gradient Boosting**, que es una técnica de ensemble learning:\n",
    "\n",
    "1. **Boosting**: Combina múltiples modelos débiles (árboles) en un modelo fuerte\n",
    "2. **Gradient**: Utiliza el gradiente de la función de pérdida para entrenar cada árbol\n",
    "3. **Secuencial**: Cada árbol se entrena para corregir los errores del anterior\n",
    "\n",
    "### 3.2 Diferencias Clave: Leaf-wise vs Level-wise\n",
    "\n",
    "- **XGBoost/otros**: Usan level-wise (crecen nivel por nivel)\n",
    "- **LightGBM**: Usa leaf-wise (crece hoja por hoja, eligiendo la hoja con mayor ganancia)\n",
    "\n",
    "**Ventaja**: Leaf-wise puede lograr menor pérdida con el mismo número de hojas, pero requiere más cuidado con el overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Ejemplo Básico con Datos Simulados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear datos de ejemplo\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "# Generar dataset sintético\n",
    "X, y = make_classification(\n",
    "    n_samples=1000,\n",
    "    n_features=20,\n",
    "    n_informative=15,\n",
    "    n_redundant=5,\n",
    "    n_classes=4,\n",
    "    n_clusters_per_class=1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "df_ejemplo = pd.DataFrame(X, columns=[f'feature_{i}' for i in range(X.shape[1])])\n",
    "df_ejemplo['target'] = y\n",
    "\n",
    "print(f\"Forma del dataset: {df_ejemplo.shape}\")\n",
    "print(f\"\\nDistribución de clases:\")\n",
    "print(df_ejemplo['target'].value_counts().sort_index())\n",
    "print(f\"\\nPrimeras filas:\")\n",
    "print(df_ejemplo.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Entrenamiento Básico de LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividir datos\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Datos de entrenamiento: {X_train.shape}\")\n",
    "print(f\"Datos de prueba: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear Dataset de LightGBM\n",
    "# LightGBM requiere un formato especial de Dataset\n",
    "train_data = lgb.Dataset(X_train, label=y_train)\n",
    "test_data = lgb.Dataset(X_test, label=y_test, reference=train_data)\n",
    "\n",
    "print(\"Datasets de LightGBM creados correctamente\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parámetros básicos de LightGBM\n",
    "params = {\n",
    "    'objective': 'multiclass',  # Para clasificación multiclase\n",
    "    'num_class': 4,  # Número de clases\n",
    "    'metric': 'multi_logloss',  # Métrica de evaluación\n",
    "    'boosting_type': 'gbdt',  # Gradient Boosting Decision Tree\n",
    "    'num_leaves': 31,  # Número de hojas (importante para controlar complejidad)\n",
    "    'learning_rate': 0.05,  # Tasa de aprendizaje\n",
    "    'feature_fraction': 0.9,  # Fracción de características a usar en cada árbol\n",
    "    'bagging_fraction': 0.8,  # Fracción de datos a usar en cada árbol\n",
    "    'bagging_freq': 5,  # Frecuencia de bagging\n",
    "    'verbose': 0  # Silenciar output\n",
    "}\n",
    "\n",
    "print(\"Parámetros configurados:\")\n",
    "for key, value in params.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenar el modelo\n",
    "model = lgb.train(\n",
    "    params,\n",
    "    train_data,\n",
    "    num_boost_round=100,  # Número de iteraciones (árboles)\n",
    "    valid_sets=[train_data, test_data],\n",
    "    valid_names=['train', 'eval'],\n",
    "    callbacks=[\n",
    "        lgb.early_stopping(stopping_rounds=10),  # Parar si no mejora en 10 rondas\n",
    "        lgb.log_evaluation(period=10)  # Mostrar métricas cada 10 rondas\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"\\nModelo entrenado correctamente\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Predicciones y Evaluación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hacer predicciones\n",
    "y_pred = model.predict(X_test, num_iteration=model.best_iteration)\n",
    "y_pred_class = np.argmax(y_pred, axis=1)  # Convertir probabilidades a clases\n",
    "\n",
    "# Evaluar\n",
    "accuracy = accuracy_score(y_test, y_pred_class)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matriz de confusión\n",
    "cm = confusion_matrix(y_test, y_pred_class)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Matriz de Confusión - LightGBM')\n",
    "plt.ylabel('Verdadero')\n",
    "plt.xlabel('Predicho')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Parámetros Principales de LightGBM\n",
    "\n",
    "### 7.1 Parámetros de Control\n",
    "\n",
    "| Parámetro | Descripción | Valores Típicos |\n",
    "|-----------|-------------|-----------------|\n",
    "| `objective` | Función objetivo | 'multiclass', 'binary', 'regression' |\n",
    "| `metric` | Métrica de evaluación | 'multi_logloss', 'binary_logloss', 'rmse' |\n",
    "| `boosting_type` | Tipo de boosting | 'gbdt', 'dart', 'goss', 'rf' |\n",
    "| `num_leaves` | Número de hojas | 31 (default), típicamente 2^max_depth - 1 |\n",
    "| `learning_rate` | Tasa de aprendizaje | 0.01 - 0.3 |\n",
    "| `num_iterations` | Número de árboles | 100 - 10000 |\n",
    "\n",
    "### 7.2 Parámetros de Regularización\n",
    "\n",
    "| Parámetro | Descripción | Efecto |\n",
    "|-----------|-------------|--------|\n",
    "| `lambda_l1` | Regularización L1 | Reduce overfitting, hace el modelo más simple |\n",
    "| `lambda_l2` | Regularización L2 | Reduce overfitting |\n",
    "| `min_gain_to_split` | Ganancia mínima para dividir | Previene divisiones inútiles |\n",
    "| `min_data_in_leaf` | Mínimo de datos en hoja | Previene overfitting |\n",
    "\n",
    "### 7.3 Parámetros de Sampling\n",
    "\n",
    "| Parámetro | Descripción |\n",
    "|-----------|-------------|\n",
    "| `feature_fraction` | Fracción de características a usar (0-1) |\n",
    "| `bagging_fraction` | Fracción de datos a usar (0-1) |\n",
    "| `bagging_freq` | Frecuencia de bagging (cada N iteraciones) |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Importancia de Características"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener importancia de características\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': [f'feature_{i}' for i in range(X.shape[1])],\n",
    "    'importance': model.feature_importance(importance_type='gain')\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"Top 10 características más importantes:\")\n",
    "print(feature_importance.head(10))\n",
    "\n",
    "# Visualizar\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.barplot(data=feature_importance.head(15), x='importance', y='feature')\n",
    "plt.title('Importancia de Características - LightGBM')\n",
    "plt.xlabel('Importancia (Gain)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Visualización del Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar un árbol (primer árbol)\n",
    "ax = lgb.plot_tree(model, tree_index=0, figsize=(20, 12))\n",
    "plt.title('Primer Árbol del Modelo LightGBM')\n",
    "plt.show()\n",
    "\n",
    "print(\"Nota: LightGBM crea muchos árboles. Este es solo el primero.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gráfico de importancia de características (método alternativo)\n",
    "lgb.plot_importance(model, max_num_features=15, figsize=(10, 8))\n",
    "plt.title('Importancia de Características - LightGBM')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Comparación con Scikit-learn API\n",
    "\n",
    "LightGBM también tiene una API compatible con scikit-learn, más fácil de usar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usando la API de scikit-learn (más simple)\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "# Crear modelo\n",
    "lgbm_classifier = LGBMClassifier(\n",
    "    objective='multiclass',\n",
    "    num_class=4,\n",
    "    num_leaves=31,\n",
    "    learning_rate=0.05,\n",
    "    n_estimators=100,\n",
    "    random_state=42,\n",
    "    verbose=-1\n",
    ")\n",
    "\n",
    "# Entrenar\n",
    "lgbm_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predecir\n",
    "y_pred_sklearn = lgbm_classifier.predict(X_test)\n",
    "\n",
    "# Evaluar\n",
    "accuracy_sklearn = accuracy_score(y_test, y_pred_sklearn)\n",
    "print(f\"Accuracy (scikit-learn API): {accuracy_sklearn:.4f}\")\n",
    "\n",
    "print(\"\\nVentajas de la API de scikit-learn:\")\n",
    "print(\"- Más familiar para usuarios de scikit-learn\")\n",
    "print(\"- Compatible con pipelines y GridSearchCV\")\n",
    "print(\"- Más simple de usar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Tuning de Hiperparámetros Básico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo de búsqueda de hiperparámetros básica\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Definir grid de parámetros\n",
    "param_grid = {\n",
    "    'num_leaves': [15, 31, 50],\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'n_estimators': [50, 100, 200]\n",
    "}\n",
    "\n",
    "# Crear modelo base\n",
    "lgbm_base = LGBMClassifier(\n",
    "    objective='multiclass',\n",
    "    num_class=4,\n",
    "    random_state=42,\n",
    "    verbose=-1\n",
    ")\n",
    "\n",
    "# Grid Search (comentado porque puede tardar)\n",
    "# grid_search = GridSearchCV(\n",
    "#     lgbm_base,\n",
    "#     param_grid,\n",
    "#     cv=3,\n",
    "#     scoring='accuracy',\n",
    "#     n_jobs=-1,\n",
    "#     verbose=1\n",
    "# )\n",
    "# \n",
    "# grid_search.fit(X_train, y_train)\n",
    "# print(f\"Mejores parámetros: {grid_search.best_params_}\")\n",
    "# print(f\"Mejor score: {grid_search.best_score_:.4f}\")\n",
    "\n",
    "print(\"Grid Search definido (descomenta para ejecutar)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Ventajas y Cuándo Usar LightGBM\n",
    "\n",
    "### Ventajas:\n",
    "1. ✅ **Velocidad**: Muy rápido, especialmente en datasets grandes\n",
    "2. ✅ **Memoria**: Uso eficiente de memoria\n",
    "3. ✅ **Precisión**: Alto rendimiento en muchos problemas\n",
    "4. ✅ **Manejo de categóricas**: Soporte nativo sin one-hot encoding\n",
    "5. ✅ **GPU**: Soporte para aceleración por GPU\n",
    "\n",
    "### Cuándo usar LightGBM:\n",
    "- ✅ Datasets grandes (>10,000 filas)\n",
    "- ✅ Muchas características\n",
    "- ✅ Necesitas velocidad\n",
    "- ✅ Problemas de clasificación o regresión\n",
    "- ✅ Datos con características categóricas\n",
    "\n",
    "### Cuándo NO usar LightGBM:\n",
    "- ❌ Datasets muy pequeños (<1,000 filas) - puede overfittear fácilmente\n",
    "- ❌ Necesitas interpretabilidad detallada (usa árboles simples)\n",
    "- ❌ Datos con muchas características irrelevantes (usa feature selection primero)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resumen\n",
    "\n",
    "En este notebook hemos aprendido:\n",
    "1. ✅ ¿Qué es LightGBM y sus ventajas\n",
    "2. ✅ Instalación y configuración básica\n",
    "3. ✅ Entrenamiento de modelos con LightGBM\n",
    "4. ✅ Parámetros principales y su significado\n",
    "5. ✅ Importancia de características\n",
    "6. ✅ API de scikit-learn vs API nativa\n",
    "7. ✅ Cuándo usar LightGBM\n",
    "\n",
    "**Próximos pasos:** En el siguiente notebook aplicaremos LightGBM al problema de predicción de fallas con datos reales."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
